---
title: "Twitter Sentiment Analysis"
author: "Kyle Amyx"
date: 2020-06-06
tags: [Python]
header:
  image: "../assets/images/twitter/twitter-header.png"
excerpt: "Data Science"
classes: wide
---

Sentiment Analysis is the process of predicting whether a piece of information(usually text) indicates
a positive, negative or neutral sentiment on a topic.

This project allows the user to input a keyword and gathers tweets based on it, then analyzes the sentiments of those tweets.


Let's start with defining the packages required for fetching and reading/writing the data.

The Python Twitter API Wrapper Docs can be found here..
https://python-twitter.readthedocs.io/en/latest/twitter.html#twitter.api.Api.GetSearch


```python
import sys
#!{sys.executable} -m pip install python-twitter
import twitter
from tqdm.notebook import tqdm
import json
import csv
import time
import random
import pickle

```

Next, we'll define the packages required for the pre-processing of the data, as well as the classification.

We will be using NLTK(Natural Language Toolkit) for the heavy lifting. The package comes with predefined `stopwords` and `punctuation`.

Stop words are generally the most common words in a language, these along with punctuation do not assist predicting the sentiment and only take up valuable space. They are removed during pre-processing


```python
#Pre-processing(NLTK -> Natural Language Toolkit)
import re
from nltk.tokenize import word_tokenize
from string import punctuation
from nltk.corpus import stopwords
import nltk
#!{sys.executable} -m pip install pattern
nltk.download('stopwords')
nltk.download('punkt')
```

    [nltk_data] Downloading package stopwords to
    [nltk_data]     C:\Users\Kyle\AppData\Roaming\nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    [nltk_data] Downloading package punkt to
    [nltk_data]     C:\Users\Kyle\AppData\Roaming\nltk_data...
    [nltk_data]   Package punkt is already up-to-date!





    True



The first step required is to connect to Twitter's developer API. This is necessary to retrieve the data we would like to test our classifier on.

You can start by applying for access at...  
[Twitter Api](!https://developer.twitter.com/en)

Once you have API credentials we can verify by caling `VerifyCredentials()`

As we can see, all my twitter information is return in response which includes a `User` object, so we're good to go.



```python


# Fun Python Trick, just print the import object to see full file path, helps with
# troubleshooting import issues.
print(twitter)
# init api instance
twitter_api = twitter.Api(consumer_key='EZy77aBssBjNigztLxn9rB3hi',
                         consumer_secret='TUJy57rUsEe0mNicUQoGgI5eUZnPQBoBE4ZjJZXqlW4VPbuY0k',
                         access_token_key='985715862207385601-Yz8Gnd0bniUrDp3n4QCgDreR52xzvEW',
                         access_token_secret='WsxdeSWj5UyNyIz4u6uEiqhxJkZXCc8iTzOooO7dZm1vo')

# for x in tqdm(range(1000000)):
#     pass

twitter_info = twitter_api.VerifyCredentials()
json_object = json.loads(str(twitter_info))
print(json.dumps(json_object, indent=2))


```

    <module 'twitter' from 'D:\\anaconda3\\lib\\site-packages\\twitter\\__init__.py'>
    {
      "created_at": "Mon Apr 16 03:05:50 +0000 2018",
      "default_profile": true,
      "description": "Ohio State Data Analytics | Android Dev",
      "favourites_count": 22,
      "followers_count": 12,
      "friends_count": 57,
      "id": 985715862207385601,
      "id_str": "985715862207385601",
      "name": "Kyle",
      "profile_background_color": "F5F8FA",
      "profile_image_url": "http://pbs.twimg.com/profile_images/1002975212260941825/TE-SZ3XW_normal.jpg",
      "profile_image_url_https": "https://pbs.twimg.com/profile_images/1002975212260941825/TE-SZ3XW_normal.jpg",
      "profile_link_color": "1DA1F2",
      "profile_sidebar_border_color": "C0DEED",
      "profile_sidebar_fill_color": "DDEEF6",
      "profile_text_color": "333333",
      "profile_use_background_image": true,
      "screen_name": "kyle_amyx",
      "status": {
        "created_at": "Fri Jun 05 22:01:16 +0000 2020",
        "favorite_count": 3,
        "id": 1269026530006650880,
        "id_str": "1269026530006650880",
        "in_reply_to_screen_name": "AZ_Truth",
        "in_reply_to_status_id": 1268918201926090752,
        "in_reply_to_user_id": 1012186134,
        "lang": "en",
        "source": "<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>",
        "text": "@AZ_Truth @SneekOut1 @Liza0311 @Mark_Cam_3 @icecube Please refer to the timestamp in which the old man acts in an a\u2026 https://t.co/XPojYHWcIH",
        "truncated": true
      },
      "statuses_count": 50
    }


Now that the API credentials have been verified, let's write a function which will fetch tweets based on a topic keyword.




```python
def getTestSet(keyword):
    try:
        tweets = twitter_api.GetSearch(keyword, count = 100, lang='en')

        print("Fetched " + str(len(tweets)) + "tweets for the term " + keyword)
        return [{"text":status.text, "label": None} for status in tweets]
    except:
        print("Error")
        return None

```

 Download test set which is just all tweets returned for a topic from TwitterAPI


```python
search_term = input("Enter word: ")
testDataSet = getTestSet(search_term)
#print(testDataSet[0:4])
for x in testDataSet[0:4]:
    print(json.dumps(x, indent=2))

```

    Enter word: blm
    Fetched 100tweets for the term blm
    {
      "text": "Florida police have released footage of the mass looting of a Walmart during the BLM race riots. https://t.co/Gvv91hMzcR",
      "label": null
    }
    {
      "text": "Imagine if Fox News was as upset about the lockdown protests as they are about BLM protests \ud83e\udd14 https://t.co/2U6BmjXJya",
      "label": null
    }
    {
      "text": "A stroke of genius cancelling the BLM protest. It gave the far-right centre stage to reveal who they are without an\u2026 https://t.co/pHiyVPxrri",
      "label": null
    }
    {
      "text": "@markgoldbridge They're gonna play with BLM on their shirts whilst playing football with said vulnerable group.\ud83d\ude02",
      "label": null
    }


NOTE: The original training dataset I chose was very skewed and contained ~4000 neutral classifications out of a dataset of 5000 points and only ~500 Positive and Negative. Therefore the model predicted EVERYTHING as neutral.


Instead I found a much more balanced training set.
http://help.sentiment140.com/for-students/

The only adjustment's needed to be made was converted the classification variable from a Integer(0,2,4) to a String(Negative, Neutral, Positive).



```python
def buildTwitterTraining():
    twitterTraining = []
    with open('twitter_training.csv', 'r', encoding='cp1252') as csvfile:
        so = csv.reader(csvfile, delimiter=',', quotechar="\"")
        for ind, row in enumerate(so):
            labelNum = row[0]
            label = ""
#         print(labelNum)
            if(labelNum == '0'):
                label = 'negative'
#             print('negative label')
            elif(labelNum == '2'):
                label = 'neutral'
#             print('neutral label')
            else:
            #labelNum == 4
                label = 'positive'
#             print('positive label')

            twitterTraining.append({"tweet_id":row[1], "text":row[5], "label":label})    
```

PrettyPrint first 10 tweets of training set to give us a preview.


```python
# buildTwitterTraining()
#Unpickle the trainingData
inFile = open('twitterTraining', 'rb')
twitterTraining = pickle.load(inFile)
inFile.close()
print(json.dumps(twitterTraining[0:10], indent=2))
```

    [
      {
        "tweet_id": "1467810369",
        "text": "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D",
        "label": "negative"
      },
      {
        "tweet_id": "1467810672",
        "text": "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!",
        "label": "negative"
      },
      {
        "tweet_id": "1467810917",
        "text": "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds",
        "label": "negative"
      },
      {
        "tweet_id": "1467811184",
        "text": "my whole body feels itchy and like its on fire ",
        "label": "negative"
      },
      {
        "tweet_id": "1467811193",
        "text": "@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. ",
        "label": "negative"
      },
      {
        "tweet_id": "1467811372",
        "text": "@Kwesidei not the whole crew ",
        "label": "negative"
      },
      {
        "tweet_id": "1467811592",
        "text": "Need a hug ",
        "label": "negative"
      },
      {
        "tweet_id": "1467811594",
        "text": "@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?",
        "label": "negative"
      },
      {
        "tweet_id": "1467811795",
        "text": "@Tatiana_K nope they didn't have it ",
        "label": "negative"
      },
      {
        "tweet_id": "1467812025",
        "text": "@twittera que me muera ? ",
        "label": "negative"
      }
    ]


Due to API rate limits, fetching all 5000 tweets takes several hours..

The next step is pre-processing of the tweets...

This is a question of what matters and what doesn't matter in Sentiment Analysis..

The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words.

Using NLTK, we can look at the pre-defined english stop words

Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens. ... Tokenization is used in computer science, where it plays a large part in the process of lexical analysis.


```python
class PreProcessTweets:
    #Constructor
    def __init__(self):
        #Create list of pre-defined stoiterablepwords + extras
        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])

    def processTweets(self, list_of_tweets):
        processedTweets=[]
        for tweet in list_of_tweets:
            processedTweets.append((self._processTweet(tweet["text"]), tweet["label"]))
        return processedTweets

    # Process Tweet -> remove all information except the tweet text(lowercased, all punctuation removed)
    def _processTweet(self, tweet):
        #Convert all letters -> lowercase
        tweet = tweet.lower()
        # remove URL's
        tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', 'URL', tweet)
        #remove usernames
        tweet = re.sub('@[^\s]+', 'AT_USER', tweet)
        #tweet = nltk.casual.remove_handles(tweet)
        #remove hash from hashtag
        tweet = re.sub(r'#([^\s]+)', r'\1', tweet)
        # tokenize words and remove repeated characters (helloooooo -> hello)
        tweet = word_tokenize(tweet)
        word_list = []
        for word in tweet:
            if(word not in self._stopwords):
                word_list.append(word)
        return word_list

```

List of pre-defined stop words from NLTK package.


```python
print(stopwords.words('english'))
```

    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]


Now to do the actual preprocessing. We create our PreProcessTweets() class and pass in our trainingSet to the `processTweets()` function. This returns a now processed training set which as discussed above will have stopwords,punctuation, usernames, url's, etc. removed. Esentailly everything that won't assist in the sentiment classification.


```python
tweetProcesser = PreProcessTweets()
# preprocessedTrainingSet = tweetProcesser.processTweets(twitterTraining)
newTraining = []
newTraining.extend(twitterTraining[0:10000])
newTraining.extend(twitterTraining[1590000:1600000])
print(newTraining[0])
preprocessedTrainingSet = tweetProcesser.processTweets(newTraining)
print(len(preprocessedTrainingSet))
print(preprocessedTrainingSet[0:10])
print('\n')
print(preprocessedTrainingSet[9990:10000])
# Each item in list is represented by [(list of tokenized words in tweet, sentiment)]
```

    {'tweet_id': '1467810369', 'text': "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D", 'label': 'negative'}
    20000
    [(['awww', "'s", 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day'], 'negative'), (['upset', 'ca', "n't", 'update', 'facebook', 'texting', '...', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah'], 'negative'), (['dived', 'many', 'times', 'ball', 'managed', 'save', '50', 'rest', 'go', 'bounds'], 'negative'), (['whole', 'body', 'feels', 'itchy', 'like', 'fire'], 'negative'), (["'s", 'behaving', "'m", 'mad', 'ca', "n't", 'see'], 'negative'), (['whole', 'crew'], 'negative'), (['need', 'hug'], 'negative'), (['hey', 'long', 'time', 'see', 'yes..', 'rains', 'bit', 'bit', 'lol', "'m", 'fine', 'thanks', "'s"], 'negative'), (['nope', "n't"], 'negative'), (['que', 'muera'], 'negative')]


    [(['wish', 'looks', 'like', 'fun'], 'negative'), (['howwww', 'read', 'thingsss', 'ahhh', 'new'], 'negative'), (['still', 'tooth', 'ache'], 'negative'), (['georgous', 'day', 'pittsburgh', 'going', 'run', 'spending', 'day', 'studying'], 'negative'), (['cant', 'people', 'get', 'head', 'ive', 'changed', 'name', 'castiel', 'windsor', 'even', 'worse', 'ca', "n't", 'even', 'spell', 'half', 'time'], 'negative'), (['aww', "'s", 'sad'], 'negative'), (['stupid', 'dvds', 'stuffing', 'good', 'bits', 'jaws'], 'negative'), (['close', 'friends', 'family', "'m", 'afraid', "'m", 'work', 'colleague'], 'negative'), (['crap', 'looking', 'last', 'tweeted', '...', 'early', "'s", '10', "n't", 'like', '...', '12'], 'negative'), (['another', 'rainboot', 'day'], 'negative')]



```python
# Pickle the OG training set, it is no longer needed since we've done the pre-processing
# twitterTrainingOutFile = open('twitterTraining', 'wb')
# pickle.dump(twitterTraining, twitterTrainingOutFile)
# twitterTrainingOutFile.close()
```

Each item in list is represented by [(list of tokenized words in tweet, sentiment)]
As shown below


```python
# del twitterTraining
# print(len(twitterTraining))
print("Frist Preprocessed Training Tweet: " + str(preprocessedTrainingSet[0]))
print("Last Preprocessed Training Tweet: " + str(preprocessedTrainingSet[19999]))
```

    Preprocessed Training Tweet: (['awww', "'s", 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day'], 'negative')
    Preprocessed Training Tweet: (['happy', 'charitytuesday'], 'positive')


Now the same preprocessing to our test set.


```python
preprocessedTestSet = tweetProcesser.processTweets(testDataSet)
```

Build a dictionary...

Compare every word in the training set against each tweet we have, creating a new binary feature which represents wether the word in our vocabulary is resident in the tweet or not


```python
def buildVocabulary(processedTrainingSet):
    all_words = []

    for (words, sentiment) in processedTrainingSet:
        all_words.extend(words)

    # Distribution of the words in all_words    
    wordlist = nltk.FreqDist(all_words)
    print(wordlist.most_common(15))
    # take top 3,000 most common words
    word_features = list(wordlist.keys())[:3000]

    return word_features


def extract_features(tweet):
    tweet_words = set(tweet)
    features = {}
    for w in word_features:
        features[w] = (w in tweet_words)
        #features['contains(%s)' % word] = (word in tweet_words)
    return features
```

So now we have a dictionary called `word_features` this contains every word in the training set and the count of its occurences, for example "(`cat`, 700)", this says the word CAT has appeared in our dataset 700 times.


```python
#Build word features set
word_features = buildVocabulary(preprocessedTrainingSet)
```

    [('...', 3583), ("'s", 2345), ("n't", 2240), ("'m", 1640), ('good', 1227), ('day', 1177), ('get', 1090), ('work', 1084), ('today', 1015), ('quot', 970), ('go', 922), ('like', 890), ('got', 848), ('going', 805), ('love', 746)]



```python
featuresets = nltk.classify.apply_features(extract_features, preprocessedTrainingSet)
#featuresets = [(extract_features(tweet), sentiment) for (tweet, sentiment) in preprocessedTrainingSet]
```

Now we've built whats called a feature set. Each item in this feature set or vector represents a tweet from the training set. Each vector item contains every word in the entire training set paired with a boolean which represents wether that word is present in the tweet, along with the pre-defined sentiment. The structure is displayed below.  


[ listOf(every word in training set : true/false(present in tweet)),    pre-defined sentiment(positive/negative) ]


Now to train the bayes classifier on the training features we've just created

The most important and the shortest part of this project, training the classifier. NLTK makes this easy with a simple function. Since our training set contained 160,000 tweets, this may take quite a bit of time to train. I personally went to work out and get some coffee so I'm sure on the exact amount of time.

It would be interesting however if there is a python package that allows some kind of wrapper around functions which counts the time until completion.


```python
NBayesClassifier = nltk.NaiveBayesClassifier.train(featuresets)
```


```python
print(NBayesClassifier)
print(len(featuresets))
```

    <nltk.classify.naivebayes.NaiveBayesClassifier object at 0x000002DBA5EDA2C8>
    20000


NOTE:

I've ran into a problem with running the classifier on my feature set which contains 160,000 individual items. I've encoutered a `Memory Error`, python tries to load the whole dataset into memory and we run out of space. Currently we have 2 options in front of us:

1.) Take a reasonable sized subset of the `FeatureSet` and run the accuracy function on this set, this will give you a good idea of the properties of the full size set

2.) Batch Processing: I'm not knowledgable enough with python to accomplish this right now, so I'll stick to option one and make the subset a size of 10,000

Below I shuffle the feature list and pull out a subset of 10,000


```python
print("Classifier accuracy percent:",(nltk.classify.accuracy(NBayesClassifier, featuresets))*100)
NBayesClassifier.show_most_informative_features(15)
```

    Classifier accuracy percent: 77.27000000000001
    Most Informative Features
                      throat = True           negati : positi =     39.7 : 1.0
                    headache = True           negati : positi =     38.3 : 1.0
                      easter = True           negati : positi =     32.3 : 1.0
                        snow = True           negati : positi =     27.9 : 1.0
                        died = True           negati : positi =     24.3 : 1.0
                      spring = True           negati : positi =     20.6 : 1.0
               unfortunately = True           negati : positi =     19.0 : 1.0
                        argh = True           negati : positi =     18.3 : 1.0
                       lines = True           positi : negati =     15.0 : 1.0
                        loss = True           negati : positi =     14.3 : 1.0
                          22 = True           negati : positi =     13.7 : 1.0
                       hates = True           negati : positi =     13.4 : 1.0
                        dang = True           negati : positi =     13.0 : 1.0
                       hurts = True           negati : positi =     12.4 : 1.0
                      tweeps = True           positi : negati =     12.3 : 1.0


The `show_most_informative_features()` function is very useful. From the output we can see that for example

`throat` appears in a negative tweet almost 40x more than in a positive.

Now we are creating a vector of predicted sentiments for each tweet in the test set.


```python
# classifing on the extracted feature vector for all the tweets in test set, which already have the sentiment labeled as NULL
NBResultLabels = []
for tweet in preprocessedTestSet:
    NBResultLabels.append(NBayesClassifier.classify(extract_features(tweet[0])))

```


```python
# MAJORITY RULE
print("Positive Labels: " + str(NBResultLabels.count('positive')))
print("Negative Labels: " + str(NBResultLabels.count('negative')))
if (NBResultLabels.count('positive') > NBResultLabels.count('negative')):
        print("POSITIVE")
        print("POSITIVE PERCENTAGE = " + str(100*NBResultLabels.count('positive')/len(NBResultLabels)) + "%")
else:
        print("NEGATIVE")
        print("NEGATIVE PERCENTAGE = " + str(100*NBResultLabels.count('negative')/len(NBResultLabels)) + "%")
```

    Positive Labels: 79
    Negative Labels: 21
    POSITIVE
    POSITIVE PERCENTAGE = 79.0%


The keyword parameter I used was `blm` which stands for `Black lives matter`, a pretty important topic at the time of this writing. It's nice to see that ~80 of the 100 tweets we fetched from the API have been classified as positive.
